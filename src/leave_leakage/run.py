from __future__ import annotations

from pathlib import Path
import pandas as pd

from leave_leakage.rules import (
    Finding,
    rule_negative_balance,
    rule_event_sign_anomaly,
    rule_taken_before_start_date,
    rule_casual_accrual_present,
    rule_balance_mismatch,
)


REQUIRED_EMP = {"employee_id", "employment_type", "fte", "start_date"}
REQUIRED_LEDGER = {"employee_id", "leave_type", "event_date", "units", "event_type"}
REQUIRED_SNAP = {"employee_id", "leave_type", "as_of_date", "balance_units"}


def _require_cols(df: pd.DataFrame, required: set[str], name: str) -> None:
    missing = sorted(required - set(df.columns))
    if missing:
        raise ValueError(f"{name} missing required columns: {missing}")


def main() -> int:
    repo_root = Path(__file__).resolve().parents[2]
    data_dir = repo_root / "data" / "sample"
    out_dir = repo_root / "outputs"
    out_dir.mkdir(parents=True, exist_ok=True)

    employees = pd.read_csv(data_dir / "employees.csv")
    ledger = pd.read_csv(data_dir / "leave_ledger.csv")
    snapshot = pd.read_csv(data_dir / "balances_snapshot.csv")

    _require_cols(employees, REQUIRED_EMP, "employees.csv")
    _require_cols(ledger, REQUIRED_LEDGER, "leave_ledger.csv")
    _require_cols(snapshot, REQUIRED_SNAP, "balances_snapshot.csv")

    # Parse dates
    ledger["event_date"] = pd.to_datetime(ledger["event_date"], errors="raise")
    snapshot["as_of_date"] = pd.to_datetime(snapshot["as_of_date"], errors="raise")

        # ----------------------------
    # Run rules
    # ----------------------------
    findings: list[Finding] = []

    findings.extend(rule_negative_balance(snapshot))
    findings.extend(rule_event_sign_anomaly(ledger))
    findings.extend(rule_taken_before_start_date(employees, ledger))
    findings.extend(rule_casual_accrual_present(employees, ledger))


    # Join ledger to snapshot on employee + leave_type, then keep events up to as_of_date
    merged = snapshot.merge(
        ledger,
        on=["employee_id", "leave_type"],
        how="left",
    )

    merged = merged[merged["event_date"].isna() | (merged["event_date"] <= merged["as_of_date"])]

    ledger_bal = (
        merged.groupby(["employee_id", "leave_type", "as_of_date"], as_index=False)["units"]
        .sum()
        .rename(columns={"units": "ledger_balance_units"})
    )

    report = snapshot.merge(
        ledger_bal,
        on=["employee_id", "leave_type", "as_of_date"],
        how="left",
    )
    report["ledger_balance_units"] = report["ledger_balance_units"].fillna(0.0)
    report["diff_units"] = report["balance_units"] - report["ledger_balance_units"]
    report["ledger_balance_units"] = report["ledger_balance_units"].round(2)
    report["diff_units"] = report["diff_units"].round(2)

    findings.extend(rule_balance_mismatch(snapshot, report))

         # ----------------------------
    # Write findings output
    # ----------------------------

    if findings:
        findings_df = pd.DataFrame([f.__dict__ for f in findings])
    else:
        findings_df = pd.DataFrame(
            columns=[
                "employee_id",
                "leave_type",
                "as_of_date",
                "rule_code",
                "severity",
                "message",
                "diff_units",
                "evidence",
            ]
        )

    findings_path = out_dir / "findings.csv"
    findings_df.to_csv(findings_path, index=False)


        # ----------------------------
    # Summary output (counts by rule and severity)
    # ----------------------------
    if len(findings_df) == 0:
        summary_df = pd.DataFrame(
            columns=["rule_code", "severity", "finding_count"]
        )
    else:
        summary_df = (
            findings_df.groupby(["rule_code", "severity"], as_index=False)
            .size()
            .rename(columns={"size": "finding_count"})
            .sort_values(["severity", "finding_count"], ascending=[True, False])
        )

    summary_path = out_dir / "summary.csv"
    summary_df.to_csv(summary_path, index=False)

    print(f"Wrote: {findings_path}")
    print(f"Wrote: {summary_path}")
    print(summary_df.to_string(index=False))


        # ----------------------------
    # Summary output (totals by severity)
    # ----------------------------
    if len(findings_df) == 0:
        severity_summary_df = pd.DataFrame(
            columns=["severity", "finding_count"]
        )
    else:
        severity_summary_df = (
            findings_df.groupby("severity", as_index=False)
            .size()
            .rename(columns={"size": "finding_count"})
            .sort_values("finding_count", ascending=False)
        )

    severity_summary_path = out_dir / "summary_by_severity.csv"
    severity_summary_df.to_csv(severity_summary_path, index=False)

    print(f"Wrote: {severity_summary_path}")
    print(severity_summary_df.to_string(index=False))  # optional


    tolerance = 0.25  # 15 minutes in hours
    report["risk_flag"] = report["diff_units"].abs() > tolerance
    report["risk_reason"] = report["risk_flag"].map(
        lambda x: "BALANCE_MISMATCH_LEDGER_VS_SNAPSHOT" if x else ""
    )

    out_path = out_dir / "leakage_report.csv"
    report.sort_values(["employee_id", "leave_type", "as_of_date"]).to_csv(out_path, index=False)

    print(f"Wrote: {out_path}")
    return 0
    

if __name__ == "__main__":
    raise SystemExit(main())
